{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07cef2b4",
   "metadata": {},
   "source": [
    "# Web Scraping Dubizzle_mobile_phones.com\n",
    "\n",
    "In this notebook, I demonstrate **two different approaches** to writing Python web scraping code:\n",
    "\n",
    "1. **Monolithic Approach (All-in-One Code)**  \n",
    "   - In this approach, the entire scraping process is written in one continuous block of code.  \n",
    "   - It includes fetching pages, parsing HTML, extracting product details, and writing to CSV all together.  \n",
    "   - This method is simple for small scripts but can become hard to maintain and read for larger projects.\n",
    "\n",
    "2. **Modular Approach (Using Functions)**  \n",
    "   - In this approach, the code is divided into **functions** for each specific task, such as:\n",
    "     - Creating the CSV file\n",
    "     - Fetching a webpage\n",
    "     - Parsing a product card\n",
    "     - Writing data to CSV\n",
    "     - Main loop controlling the scraping\n",
    "   - This method improves **readability, reusability, and maintainability**.\n",
    "   - Each function has a clear responsibility, making the code easier to debug and extend.\n",
    "\n",
    "Both methods achieve the same end result: scraping mobile phone data from the website and saving it to a CSV file.  \n",
    "The difference lies in **code organization and readability**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faca212",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e61a4d",
   "metadata": {},
   "source": [
    "## 1- Monolithic Approach (All-in-One Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700a41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa92664",
   "metadata": {},
   "source": [
    "# `Note`\n",
    "**`The code includes a random delay between each request to the server to protect and appear as a normal user, not a bot, so it can bypass the site's protection system and avoid being detected when sending multiple requests.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9599750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        ( Starting to scrape page : 1 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading ........\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ............................................            \n",
      "Successfully scraped page (1) with total (44) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 2 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................            \n",
      "Successfully scraped page (2) with total (89) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 3 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading ........\n",
      "Error scraping product: 404 Client Error: Not Found for url: https://www.dubizzle.com.eg/en/ad/%D8%AA%D9%84%D9%81%D9%88%D9%86-%D9%84%D9%84%D8%A8%D9%8A%D8%B9-%D9%82%D8%B7%D8%B9-%D8%BA%D9%8A%D8%A7%D8%B1-ID206178769.html\n",
      "Loading ...................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ....................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ..........................................           \n",
      "Successfully scraped page (3) with total (131) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 4 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading .....\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ............................................           \n",
      "Successfully scraped page (4) with total (175) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 5 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................           \n",
      "Successfully scraped page (5) with total (220) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 6 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading .\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading .......\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ..........\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ...........\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading .............\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ..............\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading .................................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading .....................................           \n",
      "Successfully scraped page (6) with total (257) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 7 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading .........\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ............................................           \n",
      "Successfully scraped page (7) with total (301) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 8 )                         \n",
      "                    ----------------------------------------                    \n",
      "\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ..........\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ........................................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ..........................................           \n",
      "Successfully scraped page (8) with total (343) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 9 )                         \n",
      "                    ----------------------------------------                    \n",
      "Loading ............................................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "           \n",
      "Successfully scraped page (9) with total (387) products.            \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 10 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................           \n",
      "Successfully scraped page (10) with total (432) products.           \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 11 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................           \n",
      "Successfully scraped page (11) with total (477) products.           \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 12 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................           \n",
      "Successfully scraped page (12) with total (522) products.           \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 13 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .............................................           \n",
      "Successfully scraped page (13) with total (567) products.           \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 14 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .....................................\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n",
      "Loading ............................................           \n",
      "Successfully scraped page (14) with total (611) products.           \n",
      "================================================================================\n",
      "\n",
      "                        ( Starting to scrape page : 15 )                        \n",
      "                    ----------------------------------------                    \n",
      "Loading .\n",
      "Error scraping product: HTTPSConnectionPool(host='www.dubizzle.com.eg', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ........"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     50\u001b[0m second_url \u001b[38;5;241m=\u001b[39m domain \u001b[38;5;241m+\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Build the full product detail URL\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m))                         \u001b[38;5;66;03m# Delay between each product to avoid being blocked\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Send GET request to the product detail page\u001b[39;00m\n\u001b[0;32m     55\u001b[0m second_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(second_url,headers\u001b[38;5;241m=\u001b[39mheaders,timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a new CSV file and write header row (column names)\n",
    "with open('dubizzle_mobile_phones.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)  # Initialize CSV writer\n",
    "    writer.writerow([\n",
    "        'product_name', 'price', 'seller', 'city', 'Governorate',\n",
    "        'Brand', 'Model', 'RAM', 'Storage', 'Battery_Capacity',\n",
    "        'Ad_Type', 'Payment_Option', 'Warranty', 'Condition', 'page_number', 'url'\n",
    "    ])  # Write the column headers in the first line\n",
    "\n",
    "page_number = 1  # Start scraping from page 1\n",
    "count = 0  # Counter to track total number of scraped products\n",
    "domain = 'https://www.dubizzle.com.eg'  # Base domain used to build full URLs\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\"AppleWebKit/537.36 (KHTML, like Gecko) \"\"Chrome/120.0 Safari/537.36\"}\n",
    "\n",
    "# Loop to scrape multiple pages\n",
    "while True:\n",
    "    url = f'https://www.dubizzle.com.eg/en/mobile-phones-tablets-accessories-numbers/mobile-phones/?page={page_number}'  # Build page URL dynamically\n",
    "    print(f'\\n{\"( Starting to scrape page : \" + str(page_number) + \" )\":^80}')  # Print current page being scraped\n",
    "    print(f'{\"-\"*40:^80}')  # Print separator line for clarity\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url,headers=headers,timeout=10)  # Send GET request to fetch page HTML\n",
    "        response.raise_for_status()  # Raise error if response status is not OK (200)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching main page {page_number}: {e}\")  # Print error message if page request fails\n",
    "        continue  # Continue scraping the next page if main page can't be fetched\n",
    "\n",
    "    items = soup.find_all('div', attrs={'class': '_4631a0ca'})  # Find all product container divs\n",
    "    if not items:  # If no items are found, it means no more pages\n",
    "        print(\"No more items found. Stopping scraper.\")  # Inform user scraping is done\n",
    "        continue \n",
    "\n",
    "    # Open CSV file in append mode to add new rows\n",
    "    with open('dubizzle_mobile_phones.csv', 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'product_name', 'price', 'seller', 'city', 'Governorate',\n",
    "            'Brand', 'Model', 'RAM', 'Storage', 'Battery_Capacity',\n",
    "            'Ad_Type', 'Payment_Option', 'Warranty', 'Condition', 'page_number', 'url'\n",
    "        ])  # Initialize DictWriter to write rows using dictionary format\n",
    "\n",
    "        dash_line = \".\"  # Used to display a loading animation\n",
    "\n",
    "        # Loop through each item (product) found on the current page\n",
    "        for item in items:\n",
    "            try:\n",
    "                link = item.find('a')  # Find the <a> tag that contains product link\n",
    "                if not link:  # If no link found, skip this product\n",
    "                    continue\n",
    "                second_url = domain + link.get('href')  # Build the full product detail URL\n",
    "                time.sleep(random.uniform(5, 10))                         # Delay between each product to avoid being blocked\n",
    "\n",
    "                \n",
    "                \n",
    "                # Send GET request to the product detail page\n",
    "                second_response = requests.get(second_url,headers=headers,timeout=10)\n",
    "                second_response.raise_for_status()  # Raise error if request failed\n",
    "                second_soup = BeautifulSoup(second_response.text, 'html.parser')  # Parse product detail page\n",
    "\n",
    "                print(f\"\\rLoading {dash_line}\", end=\"\")  # Print loading dots in one line\n",
    "                dash_line += \".\"  # Add another dot to show progress\n",
    "\n",
    "                # Extract product name safely\n",
    "                product_name = 'N/A'  # Default value\n",
    "                h1_tag = second_soup.find('h1')  # Find product title element\n",
    "                if h1_tag and re.search(r'[A-Za-z0-9]', h1_tag.get_text()):  # Check if it has text\n",
    "                    product_name = h1_tag.get_text(strip=True)  # Clean and assign name\n",
    "\n",
    "                # Extract price safely\n",
    "                price = 'N/A'  # Default value\n",
    "                price_tag = second_soup.find('span', attrs={'class': '_24469da7'})  # Find price element\n",
    "                if price_tag:  # If found\n",
    "                    price = price_tag.get_text(strip=True)  # Clean text\n",
    "\n",
    "                # Extract seller name safely\n",
    "                seller = 'N/A'  # Default value\n",
    "                seller_tag = second_soup.find('span', attrs={'class': '_9a85fb36 b7af14b4'}) or \\\n",
    "                             second_soup.find('span', attrs={'class': '_8206696c b7af14b4'})  # Try two class options\n",
    "                if seller_tag:  # If found\n",
    "                    seller = seller_tag.get_text(strip=True)  # Clean text\n",
    "\n",
    "                # Extract location (city and governorate)\n",
    "                city = 'N/A'\n",
    "                Governorate = 'N/A'\n",
    "                location_tag = second_soup.find('span', attrs={'aria-label': 'Location', 'class': 'a1c1940e'})  # Find location element\n",
    "                if location_tag:  # If found\n",
    "                    parts = [p.strip() for p in location_tag.get_text().split(',')]  # Split city and governorate by comma\n",
    "                    if len(parts) > 0:\n",
    "                        city = parts[0]  # Assign city\n",
    "                    if len(parts) > 1:\n",
    "                        Governorate = parts[1]  # Assign governorate\n",
    "\n",
    "                # Extract product specifications from details section\n",
    "                specs = {}  # Empty dictionary for specs\n",
    "                for i in second_soup.find_all('div', attrs={'class': '_92439ac7'}):  # Loop over each spec section\n",
    "                    spans = i.find_all('span')  # Find spans (key/value)\n",
    "                    if len(spans) >= 2:  # Ensure there are both key and value\n",
    "                        key = spans[0].get_text(strip=True)  # Spec name (e.g., \"Brand\")\n",
    "                        val = spans[1].get_text(strip=True)  # Spec value (e.g., \"Apple\")\n",
    "                        specs[key] = val  # Add to dictionary\n",
    "\n",
    "                # Write the extracted data into CSV file\n",
    "                writer.writerow({\n",
    "                    'product_name': product_name,\n",
    "                    'price': price,\n",
    "                    'seller': seller,\n",
    "                    'city': city,\n",
    "                    'Governorate': Governorate,\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'RAM': specs.get('RAM', 'N/A'),\n",
    "                    'Storage': specs.get('Storage', 'N/A'),\n",
    "                    'Battery_Capacity': specs.get('Battery Capacity', 'N/A'),\n",
    "                    'Ad_Type': specs.get('Ad Type', 'N/A'),\n",
    "                    'Payment_Option': specs.get('Payment Option', 'N/A'),\n",
    "                    'Warranty': specs.get('Warranty', 'N/A'),\n",
    "                    'Condition': specs.get('Condition', 'N/A'),\n",
    "                    'page_number': page_number,\n",
    "                    'url': second_url\n",
    "                })  # Write one row (product) to CSV file\n",
    "\n",
    "                count += 1  # Increment total product counter\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError scraping product: {e}\")  # Print product-specific error\n",
    "                continue  # Continue scraping next product even if one fails\n",
    "\n",
    "    print(f\"\\nSuccessfully scraped page ({page_number} of { soup.find_all('div',attrs={'title':'tiq7kl','class':'_44eaf83c'})[-1:][0].get_text() }) with total ({count}) products.\".center(80, \" \"))  # Summary per page\n",
    "    print('=' * 80)  # Separator line\n",
    "    count=0  \n",
    "    time.sleep(random.uniform(5,15))  #  Give the code a break before going to the next page (like a normal user)\n",
    "        \n",
    "    \n",
    "    # Try to find if there is a \"Next\" page\n",
    "    # If no next page exists\n",
    "    if page_number==soup.find_all('div',attrs={'title':'tiq7kl','class':'_44eaf83c'})[-1:][0].get_text():\n",
    "        print(\"No more pages found. Finished scraping.\")  # End message\n",
    "        break  # Stop the while loop\n",
    "    else:\n",
    "        page_number+=1  # Go to next page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fa14c",
   "metadata": {},
   "source": [
    "- **if need to hiden the KeyboardInterrupt error a bove use to try and (except KeyboardInterrupt )**\n",
    "- **total_products = count of all products from previous pages + count of products on the current page**\n",
    "  - If you want total_products to represent only the count of products on each individual page, then after scraping all products from the current page, reset the counter to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1ca6a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>seller</th>\n",
       "      <th>city</th>\n",
       "      <th>Governorate</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage</th>\n",
       "      <th>Battery_Capacity</th>\n",
       "      <th>Ad_Type</th>\n",
       "      <th>Payment_Option</th>\n",
       "      <th>Warranty</th>\n",
       "      <th>Condition</th>\n",
       "      <th>page_number</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Samsung A56 new</td>\n",
       "      <td>EGP 20,000</td>\n",
       "      <td>Mohamed Salah</td>\n",
       "      <td>New Nozha</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>A56</td>\n",
       "      <td>8</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Yes</td>\n",
       "      <td>New</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/samsung-a56-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>iPhone 13 Pro Max for sale</td>\n",
       "      <td>EGP 39,000</td>\n",
       "      <td>نيمو</td>\n",
       "      <td>Maryotaya</td>\n",
       "      <td>Giza</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>13 Pro Max</td>\n",
       "      <td>6</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 4000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>No</td>\n",
       "      <td>Used</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/iphone-13-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>iphone 15 pro</td>\n",
       "      <td>EGP 51,000</td>\n",
       "      <td>Adham Anbar</td>\n",
       "      <td>Sheikh Zayed</td>\n",
       "      <td>Giza</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>15 Pro</td>\n",
       "      <td>8</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 2000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>No</td>\n",
       "      <td>Used</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/iphone-15-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iphone 16 Pro Max 256 Gb Desert titanium معفي ...</td>\n",
       "      <td>EGP 56,000</td>\n",
       "      <td>LORD</td>\n",
       "      <td>Maadi</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>16 Pro Max</td>\n",
       "      <td>8</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Used</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/iphone-16-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Samsung Galaxy A16 جديد متبرشم – 256 جيجا | 8 ...</td>\n",
       "      <td>EGP 9,800</td>\n",
       "      <td>Max Trade</td>\n",
       "      <td>Sheraton</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>A16</td>\n",
       "      <td>8</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Yes</td>\n",
       "      <td>New</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/samsung-gala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>ايفون 6s plus</td>\n",
       "      <td>EGP 3,200</td>\n",
       "      <td>dykwthnksrvjofd</td>\n",
       "      <td>10th of Ramadan</td>\n",
       "      <td>Sharqia</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>6S Plus</td>\n",
       "      <td>3</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>(+) 3000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Used</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/%D8%A7%D9%8A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>ريلمى C3</td>\n",
       "      <td>EGP 2,500</td>\n",
       "      <td>ممتاز عبدالهادي محمد</td>\n",
       "      <td>Ezbet El Nakhl</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Realme</td>\n",
       "      <td>C3</td>\n",
       "      <td>3</td>\n",
       "      <td>64 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>No</td>\n",
       "      <td>Used</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/%D8%B1%D9%8A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>15 pro 128Gb ١٥ برو ١٢٨جيجا</td>\n",
       "      <td>EGP 45,500</td>\n",
       "      <td>User 92n3ha</td>\n",
       "      <td>Sheikh Zayed</td>\n",
       "      <td>Giza</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>15 Pro</td>\n",
       "      <td>6</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>(+) 3000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>No</td>\n",
       "      <td>Used</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/15-pro-128gb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>a16. للبيع سامسونج جديد</td>\n",
       "      <td>EGP 7,500</td>\n",
       "      <td>عمرو</td>\n",
       "      <td>Rod al-Farag</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>A16</td>\n",
       "      <td>8</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Yes</td>\n",
       "      <td>New</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/a16-%D9%84%D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>iPhone 11</td>\n",
       "      <td>EGP 11,500</td>\n",
       "      <td>Kareem Abbas</td>\n",
       "      <td>New Cairo</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Apple - iPhone</td>\n",
       "      <td>11</td>\n",
       "      <td>More than 16</td>\n",
       "      <td>64 GB</td>\n",
       "      <td>(+) 5000 mAH</td>\n",
       "      <td>For Sale</td>\n",
       "      <td>Cash</td>\n",
       "      <td>No</td>\n",
       "      <td>Used</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.dubizzle.com.eg/en/ad/iphone-11-ID...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          product_name       price   \n",
       "5                                      Samsung A56 new  EGP 20,000  \\\n",
       "9                           iPhone 13 Pro Max for sale  EGP 39,000   \n",
       "11                                       iphone 15 pro  EGP 51,000   \n",
       "13   Iphone 16 Pro Max 256 Gb Desert titanium معفي ...  EGP 56,000   \n",
       "14   Samsung Galaxy A16 جديد متبرشم – 256 جيجا | 8 ...   EGP 9,800   \n",
       "..                                                 ...         ...   \n",
       "571                                      ايفون 6s plus   EGP 3,200   \n",
       "574                                           ريلمى C3   EGP 2,500   \n",
       "597                        15 pro 128Gb ١٥ برو ١٢٨جيجا  EGP 45,500   \n",
       "610                            a16. للبيع سامسونج جديد   EGP 7,500   \n",
       "611                                          iPhone 11  EGP 11,500   \n",
       "\n",
       "                   seller             city Governorate           Brand   \n",
       "5           Mohamed Salah        New Nozha       Cairo         Samsung  \\\n",
       "9                    نيمو        Maryotaya        Giza  Apple - iPhone   \n",
       "11            Adham Anbar     Sheikh Zayed        Giza  Apple - iPhone   \n",
       "13                   LORD            Maadi       Cairo  Apple - iPhone   \n",
       "14              Max Trade         Sheraton       Cairo         Samsung   \n",
       "..                    ...              ...         ...             ...   \n",
       "571       dykwthnksrvjofd  10th of Ramadan     Sharqia  Apple - iPhone   \n",
       "574  ممتاز عبدالهادي محمد   Ezbet El Nakhl       Cairo          Realme   \n",
       "597           User 92n3ha     Sheikh Zayed        Giza  Apple - iPhone   \n",
       "610                  عمرو     Rod al-Farag       Cairo         Samsung   \n",
       "611          Kareem Abbas        New Cairo       Cairo  Apple - iPhone   \n",
       "\n",
       "          Model           RAM Storage Battery_Capacity   Ad_Type   \n",
       "5           A56             8  256 GB     (+) 5000 mAH  For Sale  \\\n",
       "9    13 Pro Max             6  256 GB     (+) 4000 mAH  For Sale   \n",
       "11       15 Pro             8  256 GB     (+) 2000 mAH  For Sale   \n",
       "13   16 Pro Max             8  256 GB     (+) 5000 mAH  For Sale   \n",
       "14          A16             8  256 GB     (+) 5000 mAH  For Sale   \n",
       "..          ...           ...     ...              ...       ...   \n",
       "571     6S Plus             3  128 GB     (+) 3000 mAH  For Sale   \n",
       "574          C3             3   64 GB     (+) 5000 mAH  For Sale   \n",
       "597      15 Pro             6  128 GB     (+) 3000 mAH  For Sale   \n",
       "610         A16             8  256 GB     (+) 5000 mAH  For Sale   \n",
       "611          11  More than 16   64 GB     (+) 5000 mAH  For Sale   \n",
       "\n",
       "    Payment_Option Warranty Condition  page_number   \n",
       "5             Cash      Yes       New            1  \\\n",
       "9             Cash       No      Used            1   \n",
       "11            Cash       No      Used            1   \n",
       "13            Cash      Yes      Used            1   \n",
       "14            Cash      Yes       New            1   \n",
       "..             ...      ...       ...          ...   \n",
       "571           Cash      Yes      Used           14   \n",
       "574           Cash       No      Used           14   \n",
       "597           Cash       No      Used           14   \n",
       "610           Cash      Yes       New           14   \n",
       "611           Cash       No      Used           15   \n",
       "\n",
       "                                                   url  \n",
       "5    https://www.dubizzle.com.eg/en/ad/samsung-a56-...  \n",
       "9    https://www.dubizzle.com.eg/en/ad/iphone-13-pr...  \n",
       "11   https://www.dubizzle.com.eg/en/ad/iphone-15-pr...  \n",
       "13   https://www.dubizzle.com.eg/en/ad/iphone-16-pr...  \n",
       "14   https://www.dubizzle.com.eg/en/ad/samsung-gala...  \n",
       "..                                                 ...  \n",
       "571  https://www.dubizzle.com.eg/en/ad/%D8%A7%D9%8A...  \n",
       "574  https://www.dubizzle.com.eg/en/ad/%D8%B1%D9%8A...  \n",
       "597  https://www.dubizzle.com.eg/en/ad/15-pro-128gb...  \n",
       "610  https://www.dubizzle.com.eg/en/ad/a16-%D9%84%D...  \n",
       "611  https://www.dubizzle.com.eg/en/ad/iphone-11-ID...  \n",
       "\n",
       "[110 rows x 16 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('dubizzle_mobile_phones.csv')\n",
    "df= df.dropna(inplace=True) # remove null\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5628b92",
   "metadata": {},
   "source": [
    "# `Note:`\n",
    "- **The reason for the null values is that the website detected too many frequent requests. Due to its protection mechanisms, it returned empty `Html` responses, which caused the scraper to extract no data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbed73c",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509c79f",
   "metadata": {},
   "source": [
    "# 2- Modular Approach (Using Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeea3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file and columns\n",
    "csv_file = 'dubizzle_mobile_phones_1.csv'\n",
    "fieldnames = [\n",
    "    'product_name', 'price', 'seller', 'city', 'Governorate',\n",
    "    'Brand', 'Model', 'RAM', 'Storage', 'Battery_Capacity',\n",
    "    'Ad_Type', 'Payment_Option', 'Warranty', 'Condition', 'page_number', 'url' ]\n",
    "\n",
    "# Headers for requests\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\"AppleWebKit/537.36 (KHTML, like Gecko) \"\"Chrome/120.0 Safari/537.36\"}\n",
    "\n",
    "domain = 'https://www.dubizzle.com.eg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4e8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv():\n",
    "    \"\"\"\n",
    "    Create a CSV file and write the header row.\n",
    "    \n",
    "    This function initializes the CSV file where all scraped\n",
    "    mobile phone data will be saved.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490287ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url):\n",
    "    \"\"\"\n",
    "    Fetch a webpage and return a BeautifulSoup object.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object if successful, None if request failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[Error] Failed to fetch {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603df4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_card(item, page_number):\n",
    "    \"\"\"\n",
    "    Extract product details from a product card and its detail page.\n",
    "    \n",
    "    Parameters:\n",
    "        item: BeautifulSoup element containing the product card\n",
    "        page_number (int): Current page number\n",
    "    \n",
    "    Returns:\n",
    "        dict: Product information\n",
    "        None: If the detail page could not be fetched\n",
    "    \"\"\"\n",
    "    try:\n",
    "        link_tag = item.find('a')\n",
    "        if not link_tag:\n",
    "            return None\n",
    "\n",
    "        second_url = domain + link_tag.get('href')\n",
    "        time.sleep(random.uniform(2, 7))\n",
    "        second_soup = fetch_page(second_url)\n",
    "        if second_soup is None:\n",
    "            return None\n",
    "\n",
    "        # Product Name\n",
    "        h1_tag = second_soup.find('h1')\n",
    "        product_name = h1_tag.get_text(strip=True) if h1_tag else 'N/A'\n",
    "\n",
    "        # Price\n",
    "        price_tag = second_soup.find('span', attrs={'class': '_24469da7'})\n",
    "        price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
    "\n",
    "        # Seller\n",
    "        seller_tag = second_soup.find('span', attrs={'class': '_9a85fb36 b7af14b4'}) or \\\n",
    "                     second_soup.find('span', attrs={'class': '_8206696c b7af14b4'})\n",
    "        seller = seller_tag.get_text(strip=True) if seller_tag else 'N/A'\n",
    "\n",
    "        # Location\n",
    "        city = Governorate = 'N/A'\n",
    "        location_tag = second_soup.find('span', attrs={'aria-label': 'Location', 'class': 'a1c1940e'})\n",
    "        if location_tag:\n",
    "            parts = [p.strip() for p in location_tag.get_text().split(',')]\n",
    "            if len(parts) > 0: city = parts[0]\n",
    "            if len(parts) > 1: Governorate = parts[1]\n",
    "\n",
    "        # Specifications\n",
    "        specs = {}\n",
    "        for div in second_soup.find_all('div', attrs={'class': '_92439ac7'}):\n",
    "            spans = div.find_all('span')\n",
    "            if len(spans) >= 2:\n",
    "                specs[spans[0].get_text(strip=True)] = spans[1].get_text(strip=True)\n",
    "\n",
    "        # Create a dictionary of all product information\n",
    "        product_data = {\n",
    "            'product_name': product_name,\n",
    "            'price': price,\n",
    "            'seller': seller,\n",
    "            'city': city,\n",
    "            'Governorate': Governorate,\n",
    "            'Brand': specs.get('Brand', 'N/A'),\n",
    "            'Model': specs.get('Model', 'N/A'),\n",
    "            'RAM': specs.get('RAM', 'N/A'),\n",
    "            'Storage': specs.get('Storage', 'N/A'),\n",
    "            'Battery_Capacity': specs.get('Battery Capacity', 'N/A'),\n",
    "            'Ad_Type': specs.get('Ad Type', 'N/A'),\n",
    "            'Payment_Option': specs.get('Payment Option', 'N/A'),\n",
    "            'Warranty': specs.get('Warranty', 'N/A'),\n",
    "            'Condition': specs.get('Condition', 'N/A'),\n",
    "            'page_number': page_number,\n",
    "            'url': second_url\n",
    "        }\n",
    "\n",
    "        return product_data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to parse product: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492719b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data):\n",
    "    \"\"\"\n",
    "    Write a single product's data to the CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        data (dict): Product information\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec328e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function controlling the scraping process.\n",
    "    \n",
    "    - Creates CSV\n",
    "    - Iterates through pages\n",
    "    - Scrapes product cards\n",
    "    - Writes data to CSV\n",
    "    \"\"\"\n",
    "    create_csv()\n",
    "    page_number = 1\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        url = f'{domain}/en/mobile-phones-tablets-accessories-numbers/mobile-phones/?page={page_number}'\n",
    "        soup = fetch_page(url)\n",
    "        if soup is None:\n",
    "            print(f\"Skipping page {page_number} due to fetch error.\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        print(f'( Starting to scrape page : {page_number} )'.center(80, \" \"))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        dash_line = \".\"\n",
    "        items = soup.find_all('div', attrs={'class': '_4631a0ca'})\n",
    "        if not items:\n",
    "            print(\"No more items found. Stopping scraper.\")\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            print(f\"\\rLoading {dash_line}\", end=\"\")\n",
    "            dash_line += \".\"\n",
    "            product_data = parse_product_card(item, page_number)\n",
    "            if product_data:\n",
    "                write_to_csv(product_data)\n",
    "                count += 1\n",
    "\n",
    "        print(f\"\\nSuccessfully scraped page ({page_number}) with ({count}) products.\".center(80, \" \"))\n",
    "        print(\"=\"*80)\n",
    "        count=0\n",
    "\n",
    "        # Try to find if there is a \"Next\" page\n",
    "        # If no next page exists\n",
    "        if page_number==soup.find_all('div',attrs={'title':'tiq7kl','class':'_44eaf83c'})[-1:][0].get_text():\n",
    "            print(\"No more pages found. Finished scraping.\")  # End message\n",
    "            break  # Stop the while loop\n",
    "        else:\n",
    "            page_number+=1  # Go to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Script \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
