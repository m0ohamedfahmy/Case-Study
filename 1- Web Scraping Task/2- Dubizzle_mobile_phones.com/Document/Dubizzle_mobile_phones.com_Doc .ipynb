{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59627c81",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: center; background-color: #569db3; color: white; padding: 14px; line-height: 1;border-radius:20px\">Document ‚Äî Web Scraping [Dubizzle_mobile_phones.com]</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d975",
   "metadata": {},
   "source": [
    "## üß≠ Table of Contents\n",
    "1. [‚öôÔ∏è Code Approaches](#Ô∏ècode)\n",
    "2. [üåê Which Website Was Scraped](#-which-website-was-scraped)\n",
    "3. [üíæ Data Collected](#-data-collected)\n",
    "4. [üöß Challenges and Solutions](#-challenges-and-solutions)\n",
    "5. [üìò How to Run the Script](#-how-to-run-the-script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fc41f",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; font-weight:bold; background-color:#f5f5f5; border:2px solid red; padding:10px; border-radius:10px; font-family:monospace;\">\n",
    "<code>1- Code approaches:-</code>\n",
    "</h1>\n",
    "\n",
    "In this notebook, I demonstrate **two different approaches** to writing Python web scraping code:\n",
    "\n",
    "**<span style=\"color:blue;\">1. Monolithic Approach (All-in-One Code)</span>**\n",
    "\n",
    "   - In this approach, the entire scraping process is written in one continuous block of code.  \n",
    "   - It includes fetching pages, parsing HTML, extracting product details, and writing to CSV all together.  \n",
    "   - This method is simple for small scripts but can become hard to maintain and read for larger projects.\n",
    "\n",
    "**<span style=\"color:blue;\">2. Modular Approach (Using Functions)</span>**\n",
    "   - In this approach, the code is divided into **functions** for each specific task, such as:\n",
    "     - Creating the CSV file\n",
    "     - Fetching a webpage\n",
    "     - Parsing a product card\n",
    "     - Writing data to CSV\n",
    "     - Main loop controlling the scraping\n",
    "   - This method improves **readability, reusability, and maintainability**.\n",
    "   - Each function has a clear responsibility, making the code easier to debug and extend.\n",
    "\n",
    "Both methods achieve the same end result: scraping mobile phone data from the website and saving it to a CSV file.  \n",
    "The difference lies in **code organization and readability**.\n",
    "\n",
    "\n",
    "<h1 style=\"color:red; font-weight:bold; background-color:#f5f5f5; border:2px solid red; padding:10px; border-radius:10px; font-family:monospace;\">\n",
    "<code>2- Which Website Was Scraped :- </code>\n",
    "</h1>\n",
    "\n",
    "- **Website:** [Dubizzle.com](https://www.dubizzle.com.eg)  \n",
    "- **Category:** Mobile Phones  \n",
    "- The script navigates through **all pages** of the mobile phone category.\n",
    "\n",
    "<h1 style=\"color:red; font-weight:bold; background-color:#f5f5f5; border:2px solid red; padding:10px; border-radius:10px; font-family:monospace;\">\n",
    "<code>3- Data Collected </code>\n",
    "</h1>\n",
    "\n",
    "### üß† Data Description\n",
    "\n",
    "The dataset generated by this scraper contains detailed information about each mobile phone listing from Dubizzle Egypt.\n",
    "\n",
    "Here‚Äôs what each field represents:\n",
    "\n",
    "- **`product_name`** : Name of the mobile phone listed.  \n",
    "- **`price`** : The price of the phone.  \n",
    "- **`seller`** : The name or type of the seller (individual or store).  \n",
    "- **`city`** : The city where the phone is listed.  \n",
    "- **`Governorate`** : The governorate (region) of the listing.  \n",
    "- **`Brand`** : The brand of the phone (e.g., Samsung, Apple).  \n",
    "- **`Model`** : The phone‚Äôs model name or number.  \n",
    "- **`RAM`** : The RAM capacity of the phone (e.g., 4 GB).  \n",
    "- **`Storage`** : The storage size (e.g., 128 GB).  \n",
    "- **`Battery_Capacity`** : The phone‚Äôs battery capacity (if listed).  \n",
    "- **`Ad_Type`** : Whether the listing is new, used, or other.  \n",
    "- **`Payment_Option`** : Available payment methods, if provided.  \n",
    "- **`Warranty`** : Warranty information, if applicable.  \n",
    "- **`Condition`** : Condition of the phone (new/used).  \n",
    "- **`page_number`** : The page number from which the listing was scraped.  \n",
    "- **`url`** : The full URL of the product details page.  \n",
    "\n",
    "\n",
    "\n",
    "<h1 style=\"color:red; font-weight:bold; background-color:#f5f5f5; border:2px solid red; padding:10px; border-radius:10px; font-family:monospace;\">\n",
    "<code>4- Some Challenges and Solutions :-</code>\n",
    "</h1>\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:blue;\"><code>Challenge 1:</code> Anti-scraping protection on sites like Dubizzle</span>\n",
    "\n",
    "**`Problem:`**  \n",
    "Some websites (e.g., Dubizzle) detect rapid, repeated requests coming from the same machine and treat that as automated scraping. When this happens the site may:\n",
    "\n",
    "- Slow down page responses,  \n",
    "- Serve empty or different HTML (so your scraper runs but finds no data), or  \n",
    "- Temporarily block the client for several minutes.\n",
    "\n",
    "Often, after waiting a while and trying again the scraper works normally ‚Äî the site lifted the temporary restriction.\n",
    "\n",
    "**`Solution:`**  \n",
    "To reduce the chance of being flagged as a bot, i will implement these measures:\n",
    "\n",
    "1. **Add delays between requests**  \n",
    "   Insert a pause between consecutive requests so the traffic rate looks human-like. Use randomized delays (e.g., `random.uniform(2, 6)`) rather than a fixed constant to avoid a predictable pattern.\n",
    "\n",
    "2. **Use realistic request headers**  \n",
    "   Send browser-like headers (e.g., a common `User-Agent`) to make requests appear similar to a real browser session.\n",
    "\n",
    "\n",
    "**`Why this helps:`**  \n",
    "Combining randomized delays and realistic headers makes your scraping traffic resemble a human browsing pattern, which significantly reduces the chance of triggering anti-scraping systems and getting temporarily blocked.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:blue;\"><code>Challenge 2:</code> Dynamic Class Names in HTML</span>\n",
    "\n",
    "\n",
    "\n",
    "**`Problem:`**  \n",
    "Most of the website‚Äôs elements (like `<div>` and `<a>` tags) used **Bootstrap or dynamically generated class names**.  \n",
    "This caused issues because when the website was updated, class names often changed ‚Äî which meant the scraper couldn‚Äôt find the elements anymore, and no data was returned.\n",
    "\n",
    "**`Solution:`**  \n",
    "To make the scraper more stable and update-proof:\n",
    "- I made the code depend on **static or consistent class names** whenever possible.  \n",
    "- When all class names were dynamic, I used **CSS selectors with partial matches** (e.g., selecting a part of the class name that was unlikely to change).  \n",
    "- This allowed the scraper to locate elements reliably even if the site structure changed slightly.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:blue;\"><code>Challenge 3:</code>Handling Request Failures</span>\n",
    "\n",
    "**`Problem:`** Some requests failed due to network issues or missing pages.  \n",
    "**`Solution:`** Wrapped each `requests.get()` call inside a `try-except` block to catch exceptions and continue scraping without stopping the program.\n",
    "\n",
    "### <span style=\"color:blue;\"><code>Challenge 4:</code> Missing Data in Some Products</span>\n",
    "\n",
    "**`Problem:`** Some product pages had missing fields like seller or warranty.  \n",
    "**`Solution:`** Added conditional checks (`if ... else 'N/A'`) before writing data to the CSV to prevent errors and keep the dataset consistent.\n",
    "\n",
    "### <span style=\"color:blue;\"><code>Challenge 5:</code>  Incorrect Product Name Extraction</span>\n",
    "\n",
    "\n",
    "**`Problem:`** \n",
    "During the scraping process, there was a common issue where the **product name** field sometimes did not contain the actual phone name.  \n",
    "Instead, it occasionally displayed the **seller‚Äôs  location** or another unrelated title from the page.  \n",
    "\n",
    "This happened because the `<h1>` tag used for displaying the product name was **not always consistent** ‚Äî in some listings, the same tag contained the seller‚Äôs name or irrelevant text.  \n",
    "As a result, the dataset ended up with incorrect values under the `product_name` column.\n",
    "\n",
    "\n",
    "**`Solution:`**\n",
    "To fix this problem, a **regular expression check** was added to make sure the extracted text from `<h1>` is a valid product name.  \n",
    "The regex checks if the text contains **letters or numbers** before accepting it.\n",
    "\n",
    "```python\n",
    "product_name = (\n",
    "    second_soup.find('h1').get_text()\n",
    "    if re.search(r'[A-Za-z0-9]', second_soup.find('h1').get_text())\n",
    "    else 'N/A'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2b755",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; font-weight:bold; background-color:#f5f5f5; border:2px solid red; padding:10px; border-radius:10px; font-family:monospace;\">\n",
    "<code>üìò 5- How to Run the Script and What Sites Were Scraped </code>\n",
    "</h1>\n",
    "\n",
    "## 1. Overview\n",
    "This script scrapes **mobile phone data** from the website [Dubizzle.com](https://www.dubizzle.com.eg), specifically from the **Mobile Phones** category.  \n",
    "It extracts detailed information about each phone (price, seller, condition, specifications, etc.) and saves the data into a CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How to Run the Script\n",
    "\n",
    "**Install required libraries:**\n",
    "   pip install requests beautifulsoup4\n",
    "   pip install re\n",
    "\n",
    "## üß† How the Script Works\n",
    "\n",
    "Run the notebook cell in your Python environment .  \n",
    "The script will:\n",
    "\n",
    "- Start from **page 1** of the ‚ÄúMobile Phones‚Äù category.  \n",
    "- Visit each product page to extract detailed information.  \n",
    "- Continue automatically to the next page until no more products are available.  \n",
    "\n",
    "### üóÇÔ∏è Output\n",
    "- A CSV file named **`dubizzle_mobile_phones.csv`** will be created in your working directory.  \n",
    "- Each row represents one mobile phone with its full details.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
